#!/usr/bin/env -S uv run --script
# /// script
# requires-python = "==3.13.*"
# dependencies = [
#     "gnx",
#     "jax",
#     "smalldiffusion",
# ]
#
# [tool.uv.sources]
# smalldiffusion = { git = "https://github.com/danielpfrommer-tri/smalldiffusion.git" }
# gnx = { path = "../", editable = true }
# ///

import logging

import click
import typing as tp
import rich

from pathlib import Path

import torch
import jax
import jax.numpy as jnp
import numpy as np


from gnx.core import nn
from gnx.main import setup_logging
from gnx.datasets.image import Image
from gnx.methods.diffusion import DiffusionModel, NoisingForwardProcess
from gnx.methods.integrators import Euler, DDIM, DDPM, AccelDDIM
from gnx.methods.noise_schedule import NoiseSchedule

from gnx.models.unet.diffuser import UNetDiffuser, UNetDiffuserFactory

from gnx.util.experiment.fs import FsArtifactBuilder
from gnx.util.pytorch_convert import to_nested, set_unet_diffuser_weights

logger = logging.getLogger("gnx.convert_smalldiffusion")


@click.option("--nfe", type=int, default=128)
@click.option("--sigma_min", type=float)
@click.option("--sigma_max", type=float)
@click.option("--schedule_steps", type=int)
@click.option("--beta_start", type=float)
@click.option("--beta_end", type=float)
@click.option(
    "--schedule_type", type=click.Choice(["log_linear", "sigmoid"]), required=True
)
@click.option("--repr", type=bool, default=False, is_flag=True)
@click.option("--scaled", type=bool, required=True)
@click.option("--width", type=int, required=True)
@click.option("--height", type=int, required=True)
@click.option("--integrator", "integrator_type", type=str, required=True)
@click.argument("out_path")
@click.argument("in_path")
@click.command()
def main(
    in_path,
    out_path,
    schedule_type,
    sigma_min,
    sigma_max,
    schedule_steps,
    beta_start,
    beta_end,
    scaled,
    width,
    height,
    integrator_type,
    nfe,
    repr,
):
    setup_logging()
    with open(in_path, "rb") as f:
        checkpoint = torch.load(f, map_location=torch.device("cpu"))
        raw_checkpoint = checkpoint
        checkpoint = jax.tree.map(lambda x: x.cpu().numpy(), checkpoint)

    match schedule_type:
        case "log_linear":
            schedule = NoiseSchedule.log_linear_noise(sigma_min, sigma_max)
        case "sigmoid":
            schedule = NoiseSchedule.sigmoid_noise(
                schedule_steps, beta_start, beta_end
            ).constant_variance()
        case _:
            rich.print("Unrecognized schedule.")
            return
    if scaled:
        schedule = schedule.constant_variance()
    # convert the checkpoint to a nested dictionary format
    diffuser, inferred_model_params = convert_unet(checkpoint, schedule)
    channels = inferred_model_params["in_channels"]
    # Do a quick test to ensure the diffuser works
    # identically to the smalldiffusion version
    diffuser.eval_mode()  # set to eval mode (no dropout)
    test_input = jax.random.normal(jax.random.key(42), (height, width, channels))
    t = 0.5
    sigma = schedule.sigma(t) / schedule.alpha(t)
    gt_output = test_smalldiffusion(
        raw_checkpoint, test_input, sigma,
        width=width,
        height=height,
        **inferred_model_params,
    )  # fmt: skip
    df_output = diffuser(test_input, t=jnp.array(t))
    diff = jnp.max(jnp.abs(gt_output - df_output))
    assert diff < 1e-5, f"Got a difference of {diff}"

    match integrator_type:
        case "euler":
            integrator = Euler()
        case "ddim":
            integrator = DDIM()
        case "ddpm":
            integrator = DDPM()
        case "accel":
            integrator = AccelDDIM()
        case _:
            raise ValueError(f"Unknown integrator type: {integrator_type}")

    # Make a DiffusionModel from the diffuser
    model = DiffusionModel(
        diffuser,
        NoisingForwardProcess(schedule, Image(jnp.zeros((height, width, channels)))),
        integrator=integrator,
        nfe=nfe,
    )
    builder = FsArtifactBuilder(Path(out_path), "converted", "model", "latest")
    builder.set(model).build()


def test_smalldiffusion(
    checkpoint,
    input,
    sigma,
    # from command line
    width,
    height,
    # inferred from the checkpoint
    in_channels,
    out_channels,
    model_channels,
    channel_mults,
    attn_levels,
    embed_features,
):
    from smalldiffusion.diffusion import ScheduleLogLinear
    from smalldiffusion.model_unet import Unet
    from smalldiffusion.model import Rngs as TorchRngs

    attn_res = tuple(width // (2**i) for i in attn_levels)
    model = Unet(
        width,
        in_channels,
        out_channels,
        ch=model_channels,
        ch_mult=channel_mults,
        attn_resolutions=attn_res,
        rngs=TorchRngs(42)
    )
    model.load_state_dict(checkpoint, strict=True)
    model = model.to("cpu")
    model.eval()
    input = torch.tensor(np.copy(jnp.transpose(input, (2, 0, 1))[None]))
    sigma = torch.tensor(np.copy(sigma))
    with torch.no_grad():
        output = model(input.to("cpu"), sigma.to("cpu"))
    output = output.cpu().numpy()
    output = jnp.transpose(output.squeeze(0), (1, 2, 0))
    return output


def convert_unet(checkpoint, schedule) -> tuple[UNetDiffuser[tp.Any, tp.Any], dict]:
    checkpoint = to_nested(checkpoint)
    # infer the parameters from the weight shapes
    in_channels = checkpoint["conv_in"]["weight"].shape[1]
    out_channels = in_channels
    model_channels = checkpoint["conv_in"]["bias"].shape[0]
    embed_features = list(checkpoint["sig_embed"]["mlp"].values())[-1]["bias"].shape[0]
    spatial_dims = 2
    channel_mults = []
    attention_levels = []
    # Figure out which levels have attention blocks
    for i, down_block in enumerate(checkpoint["downs"].values()):
        has_attention = False
        for sub_block in down_block["blocks"].values():
            has_attention = 1 in sub_block and "attn" in sub_block[1]
            if has_attention:
                break
        if has_attention:
            attention_levels.append(i)
        # Use the bias of the second conv of the first resblock
        # to determine the channel multiplier for this level
        channels = down_block["blocks"][0][0]["layer2"][0]["bias"].shape[0]
        channel_mult = channels / model_channels
        if int(channel_mult) == channel_mult:
            channel_mult = int(channel_mult)
        channel_mults.append(channel_mult)
    channel_mults = tuple(channel_mults)
    attention_levels = tuple(attention_levels)

    logger.info("UNet parameters:")
    params = {
        "in_channels": in_channels,
        "out_channels": out_channels,
        "model_channels": model_channels,
        "channel_mults": channel_mults,
        "attn_levels": attention_levels,
        "embed_features": embed_features,
    }
    for k, v in params.items():
        logger.info(f"    {k}: {v}")

    factory = UNetDiffuserFactory(
        channels=model_channels,
        channel_mults=channel_mults,
        attention_levels=attention_levels,
        blocks_per_level=2,
        dropout=0.1,
        snr_time_embed=True,
        film_conditioning=False,
        skip_every_block=True,
    )
    diffuser = factory.create_diffuser(
        schedule.parameterize(0, 1), jnp.zeros((32, 32, in_channels)), rngs=nn.Rngs(42)
    )
    set_unet_diffuser_weights(diffuser, checkpoint)
    num_params = nn.num_params(diffuser)
    logger.info(f"Total number of parameters: {num_params}")
    if len(jax.tree.leaves(checkpoint)) > 0:
        raise ValueError(
            f"Warning: {len(jax.tree.leaves(checkpoint))} unused weights in checkpoint."
            f"{jax.tree.map(np.shape, checkpoint)}"
        )
    return diffuser, params


# Compute the sigma-embedding as in the smalldiffusion repo


if __name__ == "__main__":
    main()
